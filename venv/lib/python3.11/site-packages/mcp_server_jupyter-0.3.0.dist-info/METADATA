Metadata-Version: 2.4
Name: mcp-server-jupyter
Version: 0.3.0
Summary: A server to manage Jupyter kernels for the MCP-Jupyter VS Code extension.
License-File: LICENSE
Author: MCP Jupyter Team
Requires-Python: >=3.10,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Requires-Dist: fastapi (>=0.111.0)
Requires-Dist: ipykernel (>=6.29.0)
Requires-Dist: jupyter-client (>=8.6.0)
Requires-Dist: jupyter-core (>=5.7.0)
Requires-Dist: mcp (>=0.1.0)
Requires-Dist: nbformat (>=5.10.0)
Requires-Dist: psutil (>=5.9.0)
Requires-Dist: pydantic (>=2.7.0)
Requires-Dist: pydantic-settings (>=2.2.0)
Requires-Dist: python-json-logger (>=2.0.7)
Requires-Dist: starlette (>=0.37.0)
Requires-Dist: structlog (>=24.1.0)
Requires-Dist: uvicorn[standard] (>=0.30.0)
Requires-Dist: websockets (>=12.0)
Description-Content-Type: text/markdown

# MCP Server Jupyter

**Stateful, Production-Ready Jupyter Notebook Execution via Model Context Protocol**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://img.shields.io/badge/tests-passing-success)](./tests/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## üöÄ Get Started in 60 Seconds

### 1. Installation
This package is designed to be installed in your Python environment, just like `ipykernel`. The accompanying VS Code extension will use the version of this server from the currently selected Python environment.

```bash
pip install mcp-server-jupyter
```

### 2. Start the Server
When you use the VS Code extension, it will automatically start this server for you. The server's output, including the startup confirmation, will appear in the "MCP Jupyter" output channel in VS Code, making it easy to diagnose any issues.

A successful startup will look like this:
```
‚úÖ MCP Jupyter Server is running!
- Listening on: websocket://127.0.0.1:3000
- Workspace root: /path/to/your/project
- Ready to accept commands for notebooks in this directory.
- Press Ctrl+C to shut down.
```

---

## ‚ú® For Data Scientists: Your AI-Powered Lab Assistant

Stop wrestling with your tools and focus on your analysis. This server transforms your Jupyter environment into a smart, resilient, and collaborative workspace. Here's how it makes your day-to-day work better:

*   **‚ö°Ô∏è Never Rerun Unnecessary Cells Again.** Tweak a data cleaning step early in your notebook? Our **Smart Sync** feature knows exactly which downstream cells depend on that change and only reruns what's necessary. Stop waiting for your whole notebook to execute and get back to your analysis faster.

*   **üìâ Stop Crashing Your Notebook with Large Outputs.** Generate massive plots or print huge logs without fear. We automatically save large outputs to separate files, keeping your notebook light, responsive, and easy to share.

*   **ü§ù Work Seamlessly Between an Agent and Your Own Edits.** Want to ask an AI agent to refactor some code, but then make a few manual tweaks yourself? Our **Handoff Protocol** ensures the agent intelligently syncs its memory to match your latest work, so you never lose your train of thought or corrupt your notebook's state.

*   **üîç Write SQL, Not Boilerplate.** Use the power of SQL to directly query your in-memory pandas DataFrames. Perform complex joins, aggregations, and filtering with simple, readable SQL instead of chaining together verbose pandas commands.

---

## üîÆ A Better Workflow for Data Science

See how these tools can accelerate a common analysis workflow. Imagine you have two CSVs, `sales.csv` and `customers.csv`.

### 1. Load and Inspect Data
First, load your data into pandas DataFrames.
```python
# In cell [1]:
import pandas as pd
sales_df = pd.read_csv('sales.csv')
customers_df = pd.read_csv('customers.csv')
```

Now, instead of printing a truncated `head()`, get a rich summary of your data without overwhelming your notebook.

```python
# MCP Tool Call:
inspect_variable("analysis.ipynb", "sales_df")

# Output:
# {
#   "name": "sales_df",
#   "type": "DataFrame",
#   "summary": "Shape: (10000, 8) | Memory: 625.1 KB",
#   "columns": ["order_id", "customer_id", "date", "product", "quantity", "price", "revenue", "region"],
#   "preview": "    order_id  customer_id        date  ... quantity   price  revenue   region\n0          1          101  2024-01-15  ...        2  150.00   300.00    North\n1          2          102  2024-01-16  ...        1   50.00    50.00    South\n..."
# }
```

### 2. Query DataFrames with SQL
Answering business questions often requires joining and aggregating data. Instead of writing complex pandas code, you can use the universal language of data: SQL.

**The Old Way (Pandas):**
```python
# In a new cell:
merged_df = pd.merge(sales_df, customers_df, on='customer_id')
regional_sales = merged_df.groupby('region').agg(
    total_revenue=pd.NamedAgg(column='revenue', aggfunc='sum'),
    unique_customers=pd.NamedAgg(column='customer_id', aggfunc='nunique')
).sort_values('total_revenue', ascending=False)
print(regional_sales)
```

**The Better Way (MCP Tool):**
```python
# MCP Tool Call:
query_dataframes("analysis.ipynb", '''
    SELECT
        c.region,
        SUM(s.revenue) as total_revenue,
        COUNT(DISTINCT c.customer_id) as unique_customers
    FROM sales_df s
    JOIN customers_df c ON s.customer_id = c.customer_id
    GROUP BY c.region
    ORDER BY total_revenue DESC
''')
```
The result is a clean, readable table, and the SQL is far more intuitive for a common data analysis task.

### 3. Let the Agent Do the Work
Need a quick overview of your dataset? Ask the agent to perform an Exploratory Data Analysis (EDA).

```python
# MCP Tool Call (example prompt to an agent):
auto_analyst("analysis.ipynb", "Perform an EDA on the sales_df DataFrame.")
```
The agent can use tools like `inspect_variable` and `query_dataframes` to generate a summary report with key statistics, identify missing values, and even create visualizations of data distributions.

---

## üõ°Ô∏è Our "Do No Harm" Philosophy
For a tool that programmatically executes code on your behalf, trust is paramount. We designed this server with a core philosophy: **Do No Harm**. Your work and your development environment should always be protected.

Here‚Äôs how we put that principle into practice:

*   **File Locking**: To prevent "split-brain" scenarios where both a human and an agent could edit a notebook simultaneously, the server uses file locks. This ensures that only one process can modify a notebook at a time, preventing data corruption and race conditions.
*   **Safe Variable Inspection**: The `inspect_variable` tool uses safe, dictionary-based lookups to retrieve variable information. It explicitly avoids using `eval()`, which can be a vector for code injection attacks.
*   **No Destructive Commands**: The tool's command set is focused on notebook operations. It does not provide general-purpose shell access or commands that could delete files or otherwise harm your workspace.
*   **Clear Provenance**: Every cell executed by the agent includes metadata about the execution, so you always have a clear audit trail of what the agent did.

---

## üß† Smart Sync: DAG-Based Execution

Traditional notebooks rerun all cells below an edit. Our Smart Sync feature analyzes your code to build a dependency graph, enabling surgical re-execution that saves time and resources.

### Three Sync Strategies
1.  **Smart Mode (Default)**: Reruns only the cells affected by a change.
2.  **Incremental Mode**: Reruns the changed cell and all subsequent cells.
3.  **Full Mode**: Reruns the entire notebook from scratch.

---

## üìö Tool Categories

This server provides a comprehensive set of tools for:
- Data Analysis
- Agent-Ready Operations
- Core Kernel and Execution Management
- Handoff Protocol for Agent-Human collaboration
- Notebook and Cell Manipulation (CRUD)
- Metadata and Variable Inspection
- Asset Management

---

## ü§ù Contributing

Contributions are welcome! Please see `CONTRIBUTING.md` for our development guide, architecture deep-dive, and contribution workflow.

---

## üìù License

Apache-2.0

